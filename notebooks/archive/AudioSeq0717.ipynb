{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "from librosa import display\n",
    "import os, re, json\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import recall_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"./VGGish\")\n",
    "sys.path.append(\"./tmp\")\n",
    "\n",
    "import vggish_slim\n",
    "import vggish_params\n",
    "import vggish_input\n",
    "from load_data import ToiletData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio and VGGish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find audio files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/data_frames/data_capture_989\\back_audio_data.wav\n"
     ]
    }
   ],
   "source": [
    "data_capture_id = 989\n",
    "data_frames_dir = \"data/data_frames/\"\n",
    "for subdir, dirs, files in os.walk(data_frames_dir):\n",
    "    if subdir.split(\"_\")[-1] == str(data_capture_id) and \\\n",
    "        'back_audio_data.wav' in files:\n",
    "        print (os.path.join(subdir, 'back_audio_data.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching audio files in C:\\Users\\Jiajun\\Desktop\\20Spring\\VGGishAudio\\data\\data_frames\n",
      "--------------------------------------------------\n",
      "Find 817 audio files\n",
      "\n",
      "File summary\n",
      "back_audio_data.wav     378\n",
      "front_audio_data.wav    378\n",
      "audio_data.wav           61\n",
      "Name: File, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "parent = os.getcwd()\n",
    "data_frames_dir = os.path.join(parent, \"data\", \"data_frames\")\n",
    "print (\"Searching audio files in {}\".format(data_frames_dir))\n",
    "print (\"-\"*50)\n",
    "\n",
    "audio_info_df = []\n",
    "for subdir, dirs, files in os.walk(data_frames_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('.wav'):\n",
    "            data_capture_id = subdir.split('\\\\')[-1]\n",
    "            data_capture_id = int(''.join([i for i in data_capture_id if i.isdigit()]))\n",
    "            filepath = os.path.join(subdir, file)\n",
    "            audio_info_df.append([filepath, data_capture_id, file])\n",
    "\n",
    "audio_info_df = pd.DataFrame(audio_info_df)\n",
    "audio_info_df.columns = ['PATH', 'Index', 'File']\n",
    "\n",
    "print (\"Find {} audio files\".format(audio_info_df.shape[0]))\n",
    "print ()\n",
    "print (\"File summary\")\n",
    "print (audio_info_df.File.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PATH</th>\n",
       "      <th>Index</th>\n",
       "      <th>File</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>C:\\Users\\Jiajun\\Desktop\\20Spring\\VGGishAudio\\d...</td>\n",
       "      <td>952</td>\n",
       "      <td>back_audio_data.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>C:\\Users\\Jiajun\\Desktop\\20Spring\\VGGishAudio\\d...</td>\n",
       "      <td>953</td>\n",
       "      <td>back_audio_data.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>C:\\Users\\Jiajun\\Desktop\\20Spring\\VGGishAudio\\d...</td>\n",
       "      <td>954</td>\n",
       "      <td>back_audio_data.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>C:\\Users\\Jiajun\\Desktop\\20Spring\\VGGishAudio\\d...</td>\n",
       "      <td>955</td>\n",
       "      <td>back_audio_data.wav</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>C:\\Users\\Jiajun\\Desktop\\20Spring\\VGGishAudio\\d...</td>\n",
       "      <td>956</td>\n",
       "      <td>back_audio_data.wav</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  PATH  Index  \\\n",
       "729  C:\\Users\\Jiajun\\Desktop\\20Spring\\VGGishAudio\\d...    952   \n",
       "731  C:\\Users\\Jiajun\\Desktop\\20Spring\\VGGishAudio\\d...    953   \n",
       "733  C:\\Users\\Jiajun\\Desktop\\20Spring\\VGGishAudio\\d...    954   \n",
       "735  C:\\Users\\Jiajun\\Desktop\\20Spring\\VGGishAudio\\d...    955   \n",
       "737  C:\\Users\\Jiajun\\Desktop\\20Spring\\VGGishAudio\\d...    956   \n",
       "\n",
       "                    File  \n",
       "729  back_audio_data.wav  \n",
       "731  back_audio_data.wav  \n",
       "733  back_audio_data.wav  \n",
       "735  back_audio_data.wav  \n",
       "737  back_audio_data.wav  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we only need back_audio_file and user index between 953 and 1000 for now\n",
    "# extract them from the audio_info_df\n",
    "\n",
    "backaudio_info_df = audio_info_df[(audio_info_df.File == \"back_audio_data.wav\") \\\n",
    "                                 & (audio_info_df.Index < 996) & (audio_info_df.Index > 950)]\n",
    "backaudio_info_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label\n",
    "with open(\"tmp/classifier.json\") as json_data_file:\n",
    "    config = json.load(json_data_file)\n",
    "\n",
    "Annotations = ToiletData(config).get_annotations('annotation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 96000\n",
    "data_captures = [i for i in backaudio_info_df.Index.values if i in Annotations.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[952, 953, 954, 955, 956, 957, 958, 959, 960, 963, 964, 965, 967, 968, 969, 970, 971, 972, 974, 976, 978, 979, 980, 981, 982, 983, 984, 985, 986, 987, 989, 990, 992, 993, 994, 995]\n"
     ]
    }
   ],
   "source": [
    "print (data_captures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "sess = tf.Session()\n",
    "vgg = CreateVGGishNetwork()\n",
    "\n",
    "class Input_Audio:\n",
    "    def __init__(self, data_captures):\n",
    "        self.data_captures = data_captures\n",
    "        self.sr = 96000\n",
    "        self.window_size = 1\n",
    "    \n",
    "    \n",
    "    def _get_windowed_samples(self, data_capture_id):\n",
    "        path = backaudio_info_df.PATH[backaudio_info_df.Index == data_capture_id]\n",
    "        assert len(path.values) == 1\n",
    "        \n",
    "        path = path.values[0]\n",
    "        x, sampling_rate = librosa.load(path, self.sr)\n",
    "        window_samples = self.window_size * self.sr\n",
    "        time_steps = x.shape[0] // window_samples               # number of time_steps\n",
    "        x = x[:time_steps * window_samples]                     # make even splices\n",
    "        windowed_x = np.array(np.split(x, time_steps))\n",
    "        \n",
    "        return windowed_x\n",
    "    \n",
    "    \n",
    "    def _get_embedding(self, data_capture_id):\n",
    "        res = []\n",
    "        windowed_x = self._get_windowed_samples(data_capture_id)\n",
    "        for sample in windowed_x:\n",
    "            res.append(EmbeddingsFromVGGish(vgg, sample, self.sr).flatten())\n",
    "        res = np.array(res)[np.newaxis, :]\n",
    "        \n",
    "        return res      # (1, time_steps, 128)\n",
    "    \n",
    "    \n",
    "    def _get_label(self, data_capture_id):\n",
    "        path = backaudio_info_df.PATH[backaudio_info_df.Index == data_capture_id]\n",
    "        assert len(path.values) == 1\n",
    "        \n",
    "        path = path.values[0]\n",
    "        x, sampling_rate = librosa.load(path, self.sr)\n",
    "        window_samples = self.window_size * self.sr\n",
    "        time_steps = x.shape[0] // window_samples\n",
    "        \n",
    "        label = np.zeros(time_steps)\n",
    "        for annotation in Annotations[data_capture_id]:\n",
    "            if annotation[-1] == 'Urination':\n",
    "                start, end = map(lambda x: int(x)/1e6, annotation[:2])\n",
    "                label[int(start):int(end)+1] = 1\n",
    "        \n",
    "        return np.array(label)\n",
    "        \n",
    "    \n",
    "    def _get_all_embeddings(self):\n",
    "        res = []\n",
    "        for data_capture_id in self.data_captures:\n",
    "            print ('updating embedding for {}'.format(data_capture_id))\n",
    "            res.append(self._get_embedding(data_capture_id))\n",
    "        \n",
    "        return res       # list: (num_of_examples, )\n",
    "    \n",
    "    \n",
    "    def _get_all_labels(self):\n",
    "        res = []\n",
    "        for data_capture_id in self.data_captures:\n",
    "            print ('updating label for {}'.format(data_capture_id))\n",
    "            res.append(self._get_label(data_capture_id))\n",
    "        \n",
    "        return res       # list: (num_of_examples, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ins = Input_Audio(data_captures)\n",
    "y = ins._get_all_labels()\n",
    "x = ins._get_all_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.LSTM(32, return_sequences=True, input_shape=[None, 128]),\n",
    "    tf.keras.layers.LSTM(64, return_sequences=True, input_shape=[None, 32]),\n",
    "    tf.keras.layers.Dropout(0.2),\n",
    "    tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(2, activation=\"softmax\"))\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(0.01),\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ind = np.random.choice(len(x), int(0.7*len(x)), replace=False)\n",
    "dev_ind = [i for i in range(len(x)) if i not in train_ind]\n",
    "\n",
    "train_X = [x[i] for i in train_ind]\n",
    "dev_X = [x[i] for i in dev_ind]\n",
    "train_y = [y[i] for i in train_ind]\n",
    "dev_y = [y[i] for i in dev_ind]\n",
    "\n",
    "def train_generator():\n",
    "    ind = 0\n",
    "    while True:\n",
    "        xbatch = train_X[ind % len(train_X)]\n",
    "        ybatch = tf.keras.utils.to_categorical(train_y[ind % len(train_y)])[np.newaxis, :]\n",
    "        ind += 1\n",
    "        yield xbatch, ybatch\n",
    "\n",
    "def validation_generator():\n",
    "    ind = 0\n",
    "    while True:\n",
    "        xbatch = dev_X[ind % len(dev_X)]\n",
    "        ybatch = tf.keras.utils.to_categorical(dev_y[ind % len(dev_y)])[np.newaxis, :]\n",
    "        ind += 1\n",
    "        yield xbatch, ybatch\n",
    "\n",
    "print ('Training example: {}'.format(len(train_X)))\n",
    "print ('Validation example: {}'.format(len(dev_X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(train_generator(), validation_data=validation_generator(),\n",
    "                   steps_per_epoch=len(train_X) // 1, validation_steps = len(dev_X) // 1,\n",
    "                   epochs=15, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation on dev set single use case\n",
    "threshold = 0.3\n",
    "accuracy, recall, precision = [], [], []\n",
    "for i in range(len(dev_ind)):\n",
    "    xbatch, ybatch = dev_X[i], dev_y[i]\n",
    "    ypred_batch = (model.predict(xbatch).reshape(-1, 2)[:, 1] > threshold).astype(int)\n",
    "    accuracy.append(np.sum(ypred_batch == ybatch) / ybatch.shape[0])\n",
    "    recall.append(recall_score(ybatch, ypred_batch))\n",
    "    precision.append(precision_score(ybatch, ypred_batch))\n",
    "    #print (np.sum(ypred_batch == ybatch) / ybatch.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ([data_captures[i] for i in train_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation on dev set overall\n",
    "threshold = 0.5\n",
    "preds, labels = [], []\n",
    "for i in range(len(dev_ind)):\n",
    "    xbatch, ybatch = dev_X[i], dev_y[i]\n",
    "    preds += list((model.predict(xbatch).reshape(-1, 2)[:, 1] > threshold))\n",
    "    labels += list(ybatch)\n",
    "\n",
    "assert len(preds) == len(labels)\n",
    "\n",
    "print (classification_report(labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation on dev set overall\n",
    "threshold = 0.7\n",
    "preds, labels = [], []\n",
    "for i in range(len(dev_ind)):\n",
    "    xbatch, ybatch = dev_X[i], dev_y[i]\n",
    "    preds += list((model.predict(xbatch).reshape(-1, 2)[:, 1] > threshold))\n",
    "    labels += list(ybatch)\n",
    "\n",
    "assert len(preds) == len(labels)\n",
    "\n",
    "print (classification_report(labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation on dev set overall\n",
    "threshold = 0.9\n",
    "preds, labels = [], []\n",
    "for i in range(len(dev_ind)):\n",
    "    xbatch, ybatch = dev_X[i], dev_y[i]\n",
    "    preds += list((model.predict(xbatch).reshape(-1, 2)[:, 1] > threshold))\n",
    "    labels += list(ybatch)\n",
    "\n",
    "assert len(preds) == len(labels)\n",
    "\n",
    "print (classification_report(labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3)\n",
    "\n",
    "axes[0].boxplot(np.array(accuracy))\n",
    "axes[1].boxplot(np.array(recall))\n",
    "axes[2].boxplot(np.array(precision))\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_eval = pd.DataFrame(np.array((accuracy, recall, precision)).transpose())\n",
    "clf_eval.index = [data_captures[i] for i in dev_ind]\n",
    "clf_eval.columns = [\"Accuracy\", \"Recall\", \"Precision\"]\n",
    "clf_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Urination Evaluation Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_regions(boolean_list):\n",
    "    #boolean_list = y[data_captures.index(data_capture_id)]\n",
    "    a_list = [i for i, x in enumerate(boolean_list) if x]\n",
    "    res = []\n",
    "    start = 0\n",
    "    i = 0\n",
    "    while i < len(a_list) - 1:\n",
    "        if a_list[i+1] - a_list[i] == 1:\n",
    "            i += 1\n",
    "        else:\n",
    "            res.append([a_list[start], a_list[i]])\n",
    "            start = i + 1\n",
    "            i += 1\n",
    "    \n",
    "    if i < len(a_list):\n",
    "        res.append([a_list[start], a_list[i]])\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_data_capture_ids = [data_captures[i] for i in dev_ind]\n",
    "dev_data_capture_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetAudio(use_i):\n",
    "    sampleRate_n = 44100\n",
    "    wav_fn  = 'May17/data/data_frames/data_capture_{}/back_audio_data.wav'.format(use_i)\n",
    "    x, fs = librosa.load(wav_fn,sr=sampleRate_n)\n",
    "\n",
    "    n_fft = 2048\n",
    "    hop_length = 512\n",
    "    n_mels = 128\n",
    "    S = librosa.feature.melspectrogram(x, sr=sampleRate_n, n_fft=n_fft, \n",
    "                                       hop_length=hop_length, \n",
    "                                       n_mels=n_mels)\n",
    "    S_DB = librosa.power_to_db(S, ref=np.max)\n",
    "    return S_DB, fs, hop_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_eval(data_capture_id):\n",
    "    # get total weight and radar energy\n",
    "    total_weight = ToiletData(config).get_total_weight_sz(data_capture_id)\n",
    "    total_weight.index = (total_weight.index - total_weight.index[0]) / 1e6\n",
    "    S_DB, fs, hop_len = GetAudio(data_capture_id)\n",
    "    \n",
    "    fig, ax = plt.subplots(3, 1, figsize = (12, 6), sharex = True)\n",
    "    ax[0].plot(total_weight)\n",
    "    ax[0].grid()\n",
    "    ax[1].plot(total_weight)\n",
    "    ax[1].grid()\n",
    "    ax[2] = librosa.display.specshow(S_DB, sr=fs, hop_length=hop_len, x_axis='time', y_axis='mel')\n",
    "    \n",
    "    \n",
    "    annotated_list = y[data_captures.index(data_capture_id)]\n",
    "    predicted_list = model.predict(x[data_captures.index(data_capture_id)]).reshape(-1, 2)[:, 1] > threshold\n",
    "    \n",
    "    for region in find_regions(annotated_list):\n",
    "        ax[0].axvspan(region[0], region[1]+1, alpha=0.5, color='red')\n",
    "    \n",
    "    for region in find_regions(predicted_list):\n",
    "        ax[1].axvspan(region[0], region[1]+1, alpha=0.5, color='orange')\n",
    "    \n",
    "    ax[0].set_title(\"Annotated: {}\".format(data_capture_id))\n",
    "    ax[0].set_ylim(total_weight.median()-1, total_weight.median()+1)\n",
    "    ax[1].set_title(\"Predicted: {}\".format(data_capture_id))\n",
    "    ax[1].set_ylim(total_weight.median()-1, total_weight.median()+1)\n",
    "    \n",
    "    ax[2].xaxis.set_major_locator(plt.MaxNLocator(30))\n",
    "    \n",
    "    fig.tight_layout(pad=0.5)\n",
    "    plt.savefig(\"eval_urine_{}\".format(ind))\n",
    "    #plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in dev_data_capture_ids:\n",
    "    plot_eval(ind)\n",
    "    #plt.savefig(\"eval_{}\".format(ind))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vggish helper\n",
    "def CreateVGGishNetwork(hop_size=0.96):   # Hop size is in seconds.\n",
    "    vggish_slim.define_vggish_slim()\n",
    "    checkpoint_path = './VGGish/vggish_model.ckpt'\n",
    "    vggish_params.EXAMPLE_HOP_SECONDS = hop_size\n",
    "  \n",
    "    vggish_slim.load_vggish_slim_checkpoint(sess, checkpoint_path)\n",
    "\n",
    "    features_tensor = sess.graph.get_tensor_by_name(\n",
    "        vggish_params.INPUT_TENSOR_NAME)\n",
    "    embedding_tensor = sess.graph.get_tensor_by_name(\n",
    "        vggish_params.OUTPUT_TENSOR_NAME)\n",
    "\n",
    "    layers = {'conv1': 'vggish/conv1/Relu',\n",
    "              'pool1': 'vggish/pool1/MaxPool',\n",
    "              'conv2': 'vggish/conv2/Relu',\n",
    "              'pool2': 'vggish/pool2/MaxPool',\n",
    "              'conv3': 'vggish/conv3/conv3_2/Relu',\n",
    "              'pool3': 'vggish/pool3/MaxPool',\n",
    "              'conv4': 'vggish/conv4/conv4_2/Relu',\n",
    "              'pool4': 'vggish/pool4/MaxPool',\n",
    "              'fc1': 'vggish/fc1/fc1_2/Relu',\n",
    "              'fc2': 'vggish/fc2/Relu',\n",
    "              'embedding': 'vggish/embedding',\n",
    "              'features': 'vggish/input_features',\n",
    "             }\n",
    "    g = tf.get_default_graph()\n",
    "    for k in layers:\n",
    "        layers[k] = g.get_tensor_by_name( layers[k] + ':0')\n",
    "    \n",
    "    return {'features': features_tensor,\n",
    "            'embedding': embedding_tensor,\n",
    "            'layers': layers,\n",
    "           }\n",
    "\n",
    "\n",
    "def EmbeddingsFromVGGish(vgg, x, sr):\n",
    "    # Produce a batch of log mel spectrogram examples.\n",
    "    input_batch = vggish_input.waveform_to_examples(x, sr)\n",
    "    # print('Log Mel Spectrogram example: ', input_batch[0])\n",
    "    \n",
    "    layer_names = vgg['layers'].keys()\n",
    "    tensors = [vgg['layers'][k] for k in layer_names]\n",
    "    \n",
    "    results = sess.run(tensors,\n",
    "                       feed_dict={vgg['features']: input_batch})\n",
    "    resdict = {}\n",
    "    for i, k in enumerate(layer_names):\n",
    "        resdict[k] = results[i]\n",
    "    \n",
    "    return resdict['embedding']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tf.keras.utils import to_categorical\n",
    "def train_generator():\n",
    "    j = 0\n",
    "    while j <= 5:\n",
    "        j += 1\n",
    "        sequence_length = np.random.randint(10, 100)\n",
    "        x_train = np.random.random((1000, sequence_length, 5))\n",
    "        # y_train will depend on past 5 timesteps of x\n",
    "        y_train = x_train[:, :, 0]\n",
    "        for i in range(1, 5):\n",
    "            y_train[:, i:] += x_train[:, :-i, i]\n",
    "        y_train = tf.keras.utils.to_categorical(y_train > 2.5)\n",
    "        print (x_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('.vggishaudio': venv)",
   "language": "python",
   "name": "python37564bitvggishaudiovenvc371676a29914be5afc6760b6c40b557"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
